{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qs: Create a daily-updated data archive of observed meteorology:\n",
    "Stakeholders are Salient's Machine Learning team and our customers\n",
    "Duration limit to complete the task is a 2 hour timeframe, enforced on the honor system\n",
    "Deadline to submit an answer is 2 weeks after receipt of this email\n",
    "For now, the archive will contain 3 different observed met station WBAN codes:\n",
    "14739 (Boston), 23169 (Las Vegas), 94846 (Chicago)\n",
    "Eventually, this system must scale to handle all >100k GHCNd stations\n",
    "Get data from NCEI, example for Boston:\n",
    "https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/USW000014739.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET ALL THE LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\varun\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET THE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTION TO DOWNLOAD DATA AND TO CHECK FOR UPDATED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ghcnd_archive(station_ids):\n",
    "    \"\"\"\n",
    "    Establishes a fresh GHCND archive from scratch for a list of stations,\n",
    "    including only specific columns: ghcn_id (STATION), DATE, precip, TMAX, and TMIN.\n",
    "    \"\"\"\n",
    "    for station_id in station_ids:\n",
    "        url = f\"{BASE_URL}{station_id}.csv\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "\n",
    "            # Use StringIO to read CSV data from the response text\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            \n",
    "            # Rename 'prcp' to 'precip' if it exists\n",
    "            if 'PRCP' in df.columns:\n",
    "                df.rename(columns={'PRCP': 'precip'}, inplace=True)\n",
    "            \n",
    "            # Filter columns to only include required ones\n",
    "            if 'STATION' in df.columns:\n",
    "                df_filtered = df[['STATION', 'DATE', 'precip', 'TMAX', 'TMIN']]\n",
    "            else:\n",
    "                df_filtered = df[['ghcn_id', 'DATE', 'precip', 'TMAX', 'TMIN']]\n",
    "            \n",
    "            # Save the filtered DataFrame to a CSV file\n",
    "            filename = f\"{station_id}_ghcnd_archive.csv\"\n",
    "            df_filtered.to_csv(filename, index=False)\n",
    "            print(f\"Archive created successfully: {filename}\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error: {e} - Unable to create archive for station {station_id}.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Error: No data found in the response for station {station_id}.\")\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Error: Failed to parse the downloaded file for station {station_id}.\")\n",
    "\n",
    "def update_ghcnd_archive(station_ids):\n",
    "    \"\"\"\n",
    "    Updates the GHCND archive with the latest data for a list of stations.\n",
    "    \"\"\"\n",
    "    for station_id in station_ids:\n",
    "        filename = f\"{station_id}_ghcnd_archive.csv\"\n",
    "        try:\n",
    "            df_existing = pd.read_csv(filename)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archive not found for station {station_id}. Run build_ghcnd_archive() first.\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Error: Archive file for station {station_id} is empty.\")\n",
    "            continue\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Error: Archive file for station {station_id} is corrupted.\")\n",
    "            continue\n",
    "\n",
    "        # Convert 'DATE' column to datetime\n",
    "        df_existing['DATE'] = pd.to_datetime(df_existing['DATE'])\n",
    "        latest_date = df_existing['DATE'].max()\n",
    "        next_date = latest_date + timedelta(days=1)\n",
    "\n",
    "        if next_date > datetime.today():\n",
    "            print(f\"Archive is already up-to-date for station {station_id}.\")\n",
    "            continue\n",
    "\n",
    "        url = f\"{BASE_URL}{station_id}.csv\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Use io.StringIO to read CSV data from the response text\n",
    "            chunksize = 10**6  # Adjust chunk size as needed\n",
    "            df_list = []\n",
    "            \n",
    "            for chunk in pd.read_csv(io.StringIO(response.text), chunksize=chunksize):\n",
    "                # Rename 'prcp' to 'precip'\n",
    "                chunk.rename(columns={'PRCP': 'precip'}, inplace=True)\n",
    "                \n",
    "                # Filter columns to only include required ones\n",
    "                if 'STATION' in chunk.columns:\n",
    "                    chunk = chunk[['STATION', 'DATE', 'precip', 'TMAX', 'TMIN']]\n",
    "                else:\n",
    "                    chunk = chunk[['ghcn_id', 'DATE', 'precip', 'TMAX', 'TMIN']]\n",
    "                \n",
    "                # Convert 'DATE' column to datetime\n",
    "                chunk['DATE'] = pd.to_datetime(chunk['DATE'])\n",
    "                \n",
    "                # Filter for new data only\n",
    "                df_new_filtered = chunk[chunk['DATE'] >= next_date]\n",
    "                if not df_new_filtered.empty:\n",
    "                    df_list.append(df_new_filtered)\n",
    "\n",
    "            if df_list:\n",
    "                df_new_combined = pd.concat(df_list)\n",
    "                df_updated = pd.concat([df_existing, df_new_combined])\n",
    "                df_updated.to_csv(filename, index=False)\n",
    "                print(f\"Archive updated successfully: {filename}\")\n",
    "            else:\n",
    "                print(f\"No new data available for station {station_id}.\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error: {e} - Unable to update archive for station {station_id}.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Error: No data found in the downloaded file for station {station_id}.\")\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Error: Failed to parse the downloaded file for station {station_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALL build_ghcnd_archive FUNCTION TO DOWNLOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_11504\\1999415815.py:13: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,37,39,41,43,45,47,49,51,53,55,57,61,63,65,67,69,73,83,89,91,93,95,97,101,105,107,109) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive created successfully: USW00014739_ghcnd_archive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_11504\\1999415815.py:13: DtypeWarning: Columns (17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive created successfully: USW00094728_ghcnd_archive.csv\n",
      "Archive created successfully: USW00014819_ghcnd_archive.csv\n"
     ]
    }
   ],
   "source": [
    "# List of station IDs (example: Boston, New York Central Park, Chicago)\n",
    "station_ids = [\"USW00014739\", \"USW00094728\", \"USW00014819\"]\n",
    "# Call function to download data\n",
    "build_ghcnd_archive(station_ids)  # Run this once to create the archives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCHEDULE TASK EVERY HOUR TO CHECK IF DATA IS AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "\n",
    "def job():\n",
    "    update_ghcnd_archive(station_ids)\n",
    "\n",
    "# Schedule the job to run daily\n",
    "schedule.every().hour.do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(30)  # Wait a minute before checking again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
